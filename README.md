# NLP-Abbreviated-Words

<ol>
<li><b><code>LSTM</code></b>:Long Short Term Memory</li>
<li><b><code>Bert</code></b>: Bidirectional Encoder Representations from Transformers.</li>
<li><b><code>POS</code></b>: parts of speech.</li>
<li><b><code>DTM</code></b>: Document Term Matrix.</li>
<li><b><code>NER</code></b>: name entity recognition.</li>
<li><b><code>NLG</code></b>: Natural Language Generation.</li>
<li><b><code>NLU</code></b>: Natural Language Understanding.</li>
<li><b><code>TF IDF</code></b>: Term Frequencyâ€“Inverse Document Frequency.</li>
<li><b><code>re</code></b>: Regular expression.</li>
<li><b><code>LDA</code></b>: Latent Dirichlet Allocation.</li>
<li><b><code>LSI</code></b>: Latent Semantic Indexing.</li>
<li><b><code>NMF</code></b>: Non-Negative Matrix Factorization.</li>
<li><b><code>NLTK</code></b>: Natural Language Toolkit</li>
<li><b><code>ELMo</code></b> - Embeddings from Language Models</li>
<li><b><code>GloVe</code></b> - Global Vectors for Word Representation</li>
<li><b><code>ULMFiT</code></b> - Universal Language Model Fine-Tuning</li>
<li><b><code>Transformer-XL</code></b> - Transformer Extra Long</li>
<li><b><code>RoBERTa</code></b> - Robustly Optimized BERT Pretraining Approach</li>
<li><b><code>ERNIE </code></b>- Enhanced Representation through kNowledge IntEgration</li>
<li><b><code>GPT-2 </code></b>- Generative Pretrained Transformer - 2</li>
<li><b><code>MT-DNN a.k.a BigBird</code></b> - Multi-Task Deep Neural Network</li>
<li><b><code>PEGASUS</code></b> - Pre-training with Extracted Gap-Sentences for Abstractive Summarization</li>
<li><b><code>ALBERT</code></b> - A Lite BERT</li>
<li><b><code>ELECTRA</code></b> - Efficiently Learning an Encoder that Classifies Token Replacements Accurately</li>
<li><b><code>T5</code></b> - Text-To-Text Transfer Transformer</li>
<li><b><code>DAAF</code></b> - Data Augmentation and Auxiliary Feature</li>

</ol>
